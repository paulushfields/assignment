{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Data Exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FROM DATA TO INSIGHTS\r\n",
    "\r\n",
    "## Introduction\r\n",
    "This notebook is created that it should be possible to run it in one go.\r\n",
    "Python 3, conda and pip should be installed upfront."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python --version\r\n",
    "!conda --version\r\n",
    "!pip --version"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install whatever packages that are needed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install folium==0.12.1\r\n",
    "!pip install matplotlib==3.4.3\r\n",
    "!pip install numpy==1.21.2\r\n",
    "!pip install pandas==1.3.2\r\n",
    "!pip install requests==2.26.0\r\n",
    "!pip install scikit-learn==0.24.2\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import folium\r\n",
    "import json\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import random\r\n",
    "import requests\r\n",
    "\r\n",
    "from IPython.display import display\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "from sklearn.cluster import DBSCAN\r\n",
    "from sklearn.cluster import AffinityPropagation\r\n",
    "from sklearn.cluster import AgglomerativeClustering\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_FILE = \"global_cities_data_set.json\"\r\n",
    "URL_FILE = \"https://iisbvicmidlprdsa.blob.core.windows.net/fileshare/DATA_SET_DS_USE_CASE/global_cities_data_set.json?sv=2019-02-02&st=2021-08-06T08%3A18%3A35Z&se=2021-10-07T08%3A18%3A00Z&sr=b&sp=r&sig=vMOCDzuXhxSM%2BT02Wv3Zm2oW7BsXME2mZCk%2F%2BI5uMSU%3D\"\r\n",
    "\r\n",
    "START_FROM_SCRATCH = True\r\n",
    "\r\n",
    "# Filters\r\n",
    "REGION_FILTER = 'EUREG'\r\n",
    "\r\n",
    "YEAR_LIST = [2018, 2019, 2020, 2021, 2022, 2023, 2024]\r\n",
    "\r\n",
    "# Clustering hyper parameters\r\n",
    "EPS_VALUE = 0.02\r\n",
    "MIN_SAMPLES_VALUE = 50\r\n",
    "N_CLUSTERS = 10\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DTYPES_DICT = {\r\n",
    "    'year': np.int32,\r\n",
    "    'indicator_name': object,\r\n",
    "    'geography_iso': object,\r\n",
    "    'geography_country': object,\r\n",
    "    'geographyid': object,\r\n",
    "    'geographyname': object,\r\n",
    "    'value_unit': object,\r\n",
    "    'databank': object,\r\n",
    "    'value': np.float64\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "FILE_LIST = [\r\n",
    "    'Consumer spending by product',\r\n",
    "    'Population',\r\n",
    "    'Household numbers by income band'\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def download_and_read_source_data():\r\n",
    "    if START_FROM_SCRATCH:\r\n",
    "        r = requests.get(URL_FILE)\r\n",
    "        open(DATA_FILE, 'wb').write(r.content)\r\n",
    "\r\n",
    "    file_object = open(DATA_FILE, encoding='utf8')\r\n",
    "    data = json.load(file_object)\r\n",
    "\r\n",
    "    df = pd.json_normalize(data['data'])\r\n",
    "\r\n",
    "    print(\"df.shape: (all): \", df.shape)\r\n",
    "    # Make sure the year field is an integer\r\n",
    "    df.year = df.year.astype('int32')\r\n",
    "    \r\n",
    "    file_object.close()\r\n",
    "\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filtering\r\n",
    "\r\n",
    "In the current setup it's only possible to visualize the data for EU region,\r\n",
    "\r\n",
    "The geographyid is unique for all countries except for the USA. Therefore creating a combined logical key named geography_region_id consisting of geographyid and geographyname which is 100% unique for the region.\r\n",
    "year combined with geography_region_id is a primary key which can be used to merge data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def filter_data(par_df, par_year):\r\n",
    "    par_df[\"geography_region_key\"] = par_df[\"geographyid\"] + \"_\" + par_df[\"geographyname\"]\r\n",
    "    par_df = par_df[(par_df['databank'] == 'EUREG') & (par_df['year'] == par_year)]\r\n",
    "    print(\"par_df.shape: (\" + REGION_FILTER +  \" & \" + str(par_year) +  \"): \", par_df.shape)\r\n",
    "    return par_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indicators\r\n",
    "\r\n",
    "The file provided hosts a number of different types of data as can be seen in the indicator_name field.\r\n",
    "Some indicators belong together. For example Population per age range.\r\n",
    "These indicator_groups are handled separately.\r\n",
    "\r\n",
    "Singular indicator are written into separate files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def split_indicators(par_data_dir_name, par_df_data):\r\n",
    "    #Some indicator are grouped\r\n",
    "    indicator_groups = [\r\n",
    "        'Household numbers by income band',\r\n",
    "        'Population',\r\n",
    "        'Consumer spending by product'\r\n",
    "    ]\r\n",
    "\r\n",
    "    indicator_groups_strings = (\r\n",
    "        'Household numbers by income band',\r\n",
    "        'Population',\r\n",
    "        'Consumer spending by product'\r\n",
    "    )\r\n",
    "\r\n",
    "    other_indicators = []\r\n",
    "\r\n",
    "    for word in par_df_data.indicator_name.unique()[:]:\r\n",
    "        if not word.startswith(indicator_groups_strings):\r\n",
    "            other_indicators.append(word)\r\n",
    "\r\n",
    "    # Create separate files for indicators.\r\n",
    "    for indicator in other_indicators:\r\n",
    "        df_filtered = par_df_data[(par_df_data['indicator_name'] == indicator)]\r\n",
    "        filtered_file_name = \\\r\n",
    "            par_data_dir_name + os.path.sep + indicator.replace(\" \", \"_\"). \\\r\n",
    "        replace(\",\", \"_\").replace(\"/\", \"_\") + '.csv'\r\n",
    "        df_filtered.to_csv(filtered_file_name, sep=\";\", encoding=\"utf-8\")\r\n",
    "\r\n",
    "    # Group some indicators into one file.\r\n",
    "    for indicator_group in indicator_groups:\r\n",
    "        df_filtered = par_df_data[(\r\n",
    "            par_df_data['indicator_name'].str.startswith(indicator_group))]\r\n",
    "        filtered_file_name = \\\r\n",
    "            par_data_dir_name + os.path.sep + indicator_group + '.csv'\r\n",
    "        df_filtered.to_csv(filtered_file_name, sep=\";\", encoding=\"utf-8\")\r\n",
    "\r\n",
    "    par_df_data.to_csv(par_data_dir_name + os.path.sep + \"total_set.csv\",\r\n",
    "                       sep=\";\",\r\n",
    "                       encoding=\"utf-8\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indicator groups\r\n",
    "\r\n",
    "Now process the indicator groups. Different bands of the same kind of data are put into one file for further processing.\r\n",
    "\r\n",
    "As the value_unit might not be the same we can't compare the data in its original form.\r\n",
    "For each band a ratio is calculated to indicate what proportion of total this band represents.\r\n",
    "This makes it possible to compare the data no matter the country."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_grouped_indicator_files(par_data_dir_name, par_file_item):\r\n",
    "\r\n",
    "    df_data = pd.read_csv(\r\n",
    "        par_data_dir_name + os.path.sep + par_file_item + \".csv\",\r\n",
    "        sep=\";\",\r\n",
    "        encoding=\"utf8\",\r\n",
    "        dtype=DTYPES_DICT)\r\n",
    "\r\n",
    "    print(\"shape: \", df_data.shape)\r\n",
    "\r\n",
    "    # Remove unwanted columns when grouping\r\n",
    "    df_sum = df_data.loc[:, (\"geography_region_key\", \"year\", \"value\")]\r\n",
    "\r\n",
    "    # Sum values\r\n",
    "    df_grouped = df_sum.groupby(by=['year', 'geography_region_key']).sum()\r\n",
    "    # Back to a data frame\r\n",
    "    df_sum = df_grouped.reset_index()\r\n",
    "\r\n",
    "    def calculate_ratio(par_year, par_geography_region_key, par_value):\r\n",
    "        df_filtered_sum = df_sum[(df_sum['year'] == par_year) &\r\n",
    "            (df_sum['geography_region_key'] == par_geography_region_key)].sum()\r\n",
    "        return par_value / df_filtered_sum.values[2]\r\n",
    "\r\n",
    "    df_data['ratio'] = df_data.apply(\r\n",
    "            lambda row : calculate_ratio(\r\n",
    "                row['year'],\r\n",
    "                row['geography_region_key'],\r\n",
    "                row['value']), axis = 1)\r\n",
    "\r\n",
    "    df_data['ratio'].fillna(0, inplace=True)\r\n",
    "    \r\n",
    "    print(\"shape: \", df_data.shape)\r\n",
    "\r\n",
    "    df_data.to_csv(par_data_dir_name + os.path.sep + file_item + \"_ext.csv\",\r\n",
    "        sep=\";\",\r\n",
    "        encoding=\"utf8\")\r\n",
    "\r\n",
    "    print(\"End \" + file_item)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_rows_with_grouped_indicators(par_data_dir_name, par_file_item):\r\n",
    "\r\n",
    "    df_data = pd.read_csv(\r\n",
    "        par_data_dir_name + os.path.sep + par_file_item + \"_ext.csv\",\r\n",
    "        sep=\";\",\r\n",
    "        encoding=\"utf8\")\r\n",
    "\r\n",
    "    print(\"shape: \", df_data.shape)\r\n",
    "\r\n",
    "    column_names = []\r\n",
    "    df_data_ext = pd.DataFrame()\r\n",
    "\r\n",
    "    indicator_names = df_data.indicator_name.unique()\r\n",
    "    for indicator_name in indicator_names:\r\n",
    "        df_select = df_data[df_data.indicator_name == indicator_name] \r\n",
    "        column_name = indicator_name. \\\r\n",
    "            replace(\"resident\", \"\"). \\\r\n",
    "            replace(\"based\", \"\"). \\\r\n",
    "            replace(\"current\", \"\"). \\\r\n",
    "            replace(\"prices\", \"\"). \\\r\n",
    "            replace(\"(\", \"\"). \\\r\n",
    "            replace(\")\", \"\"). \\\r\n",
    "            replace(\"Consumer spending by product / service - \", \"\"). \\\r\n",
    "            replace(\"Household numbers by income band - \", \"\"). \\\r\n",
    "            replace(\",\", \"\"). \\\r\n",
    "            replace(\" \", \"_\"). \\\r\n",
    "            replace(\"-\", \"_\"). \\\r\n",
    "            replace(\"____\", \"\"). \\\r\n",
    "            replace(\"__\", \"_\"). \\\r\n",
    "            lower()\r\n",
    "        #print(\"column_name: \", column_name)\r\n",
    "        column_names.append(column_name)\r\n",
    "        df_select[column_name] = df_select['ratio']\r\n",
    "        df_select = df_select.loc[:, (\"geographyid\", \"geography_region_key\", \"year\", column_name)]\r\n",
    "\r\n",
    "        if (len(df_data_ext) == 0):\r\n",
    "            df_data_ext = df_select\r\n",
    "        else:\r\n",
    "            df_data_ext = df_data_ext.merge(\r\n",
    "                right=df_select,\r\n",
    "                on=[\"geographyid\", \"geography_region_key\", \"year\"],\r\n",
    "                how=\"outer\")\r\n",
    "\r\n",
    "        #print(\"Shape: \", df_data_ext.shape)\r\n",
    "\r\n",
    "    df_data_ext.fillna(0, inplace=True)\r\n",
    "\r\n",
    "    df_data_ext.to_csv(par_data_dir_name + os.path.sep + par_file_item + \"_ext2.csv\",\r\n",
    "        sep=\";\",\r\n",
    "        encoding=\"utf8\")\r\n",
    "\r\n",
    "    print(\"End \" + file_item)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocess data\r\n",
    "\r\n",
    "Download the data and preprocess it for each year."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def preprocess_data(par_data_dir_name, par_file_item, par_df_filtered):\r\n",
    "    print(\"Process :\", par_file_item)\r\n",
    "\r\n",
    "    split_indicators(par_data_dir_name, df_filtered)\r\n",
    "    generate_grouped_indicator_files(par_data_dir_name, file_item)\r\n",
    "    generate_rows_with_grouped_indicators(par_data_dir_name, file_item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data exploration\r\n",
    "\r\n",
    "Now we have a set of different files, one file for each indicator(group). Let's look at the data in more detail.\r\n",
    "\r\n",
    "### Primary data points\r\n",
    "\r\n",
    "Some of the data are the primary datapoints. These can be divided into grouped and non-grouped indicators.\r\n",
    "\r\n",
    "\r\n",
    "### Non-grouped indicators:\r\n",
    "| indicator_name                                                        | indicator_type | value_unit      | value_type | regions          | comment                        |   |\r\n",
    "|-----------------------------------------------------------------------|-----------------|-----------------|------------|------------------|--------------------------------|---|\r\n",
    "| Average_household_size                                                | demographics    | #Persons        | float      | AFR, EUREG, GCFS |                                |   |\r\n",
    "| Births                                                                | demographics    | #Persons        | float      | AFR, GCFS        | how to interpret? Aggregations |   |\r\n",
    "| CREA_house_price_index                                                | housing         | Index           | float      | AMREG            | CAN                            |   |\r\n",
    "| Deaths                                                                | demographics    | #Persons        | float      | AFR, GCFS        | how to interpret?              |   |\r\n",
    "| Employment_-_Industry                                                 | employment      | #Persons        | float      | AFR, GCFS        | not complete, how to interpret |   |\r\n",
    "| Employment_-_Transport__storage__information_&_communication_services | employment      | #Persons        | float      | AFR, GCFS        | how to interpret, not complete |   |\r\n",
    "| Gross_domestic_product__real                                          | gdp             | currency        | float      | EUREG, AMREG     |                                |   |\r\n",
    "| Homeownership_rate                                                    | housing         | %               | float      | AMREG            | USA                            |   |\r\n",
    "| Household_disposable_income__per_household__nominal                   | housing         | currency        | float      | EUREG            |                                |   |\r\n",
    "| Household_disposable_income__per_household__real                      | housing         | currency        | float      | EUREG            |                                |   |\r\n",
    "| Household_disposable_income__real                                     | housing         | currency        | float      | EUREG            |                                |   |\r\n",
    "| Housing_permits_-_multi_family                                        | housing         | Housing permits | float      | AMREG            | USA                            |   |\r\n",
    "| Housing_permits_-_single_family                                       | housing         | Housing permits | float      | AMREG            | USA                            |   |\r\n",
    "| Housing_permits_-_total                           | housing      | Housing permits | float | AMREG     | USA                               |   |\r\n",
    "| Housing_starts                                    | housing      | null            | float | AMREG     | CAN, how to interpret?            |   |\r\n",
    "| Housing_starts_-_multi_family                     | housing      | Housing starts  | float | AMREG     | USA                               |   |\r\n",
    "| Housing_starts_-_single_family                    | housing      | Housing starts  | float | AMREG     | USA                               |   |\r\n",
    "| Housing_starts_-_total                            | housing      | Housing starts  | float | AMREG     | USA                               |   |\r\n",
    "| Income_from_employment__nominal                   | income       | currency        | float | AMREG     | USA                               |   |\r\n",
    "| Income_from_rent__dividends_and_interest__nominal | income       | currency        | float | AMREG     | USA                               |   |\r\n",
    "| Income_taxes__nominal                             | income       | currency        | float | AMREG     | USA                               |   |\r\n",
    "| Labor_force                                       | employment   | #Persons        | float | AMREG     | USA, CAN                          |   |\r\n",
    "| Labor_force_participation_rate                    | employment   | %               | float | AMREG     | USA                               |   |\r\n",
    "| Labour_force_participation_rate                   | employment   | %               | float | AMREG     | CAN                               |   |\r\n",
    "| Median_household_income__real                     | income       | currency        | float | AMREG     | USA                               |   |\r\n",
    "| Net_migration_(including_statistical_adjustment)  | demographics | #Persons        | float | AFR, GCFS | can be both negative and positive |   |\r\n",
    "| New_housing_price_index                           | housing      | index           | float | AMREG     | CAN                               |   |\r\n",
    "| Personal_disposable_income__per_capita__real      | income       | currency        | float | AMREG     | USA, CAN                          |   |\r\n",
    "| Personal_disposable_income__per_household__real   | income       | currency        | float | AMREG     | USA, CAN                          |   |\r\n",
    "| Personal_income__per_capita__real    | income       | currency    | float | AMREG | USA, CAN |   |\r\n",
    "| Personal_income__per_household__real | income       | currency    | float | AMREG | USA, CAN |   |\r\n",
    "| Proprietors_incomes__nominal         | income       | currency    | float | AMREG | USA      |   |\r\n",
    "| Residential_building_permits         | housing      | null        | float | AMREG | CAN      |   |\r\n",
    "| Social_security_payments__nominal    | income       | currency    | float | AMREG | USA      |   |\r\n",
    "| Total_households                     | housing      | #Households | float | All   |          |   |\r\n",
    "| Total_population                     | demographics | #Persons    | float | All   |          |   |\r\n",
    "| Unemployment_level                   | unemployment | #Persons    | float | AMREG | USA, CAN |   |\r\n",
    "| Unemployment_rate                    | unemployment | %           | float | AMREG | USA, CAN |   |\r\n",
    "| Urban_Total_Population               | demographics | #Persons    | float | All   |          |   |\r\n",
    "\r\n",
    "<br/>\r\n",
    "\r\n",
    "### Grouped indicators\r\n",
    "\r\n",
    "| indicator_name                    | indicator_type  | value_unit  | value_type | regions | comment                                             |\r\n",
    "|-----------------------------------|-----------------|-------------|------------|---------|-----------------------------------------------------|\r\n",
    "| Population*                       | demographics    | #Persons    | float      | All     |                                                     |\r\n",
    "| Consumer spending by product*     | spending        | currency    | float      | All     | value_unit contains : empty, null                   |\r\n",
    "| Household numbers by income band* | income          | #Households | float      | All     | value contains float values very big and very small |\r\n",
    "\r\n",
    "<br/>\r\n",
    "\r\n",
    "## Secondary data points\r\n",
    "\r\n",
    "There's a set of secondary data points that describe the primary data points in terms of a number of facets. For instance geographical region, year etc.\r\n",
    "\r\n",
    "| indicator_name    | value_unit | value_type | key  | comment                    |\r\n",
    "|-------------------|------------|------------|------|----------------------------|\r\n",
    "| year              | year       | int        | Key1 |                            |\r\n",
    "| geography_iso     | category   | string     |      | ISO 3166-1 alpha-3         |\r\n",
    "| geography_country | category   | string     |      |                            |\r\n",
    "| geographyid       | category   | string     | Key2 | NUTS-2 region data (EUREG), No standards found for other regions |\r\n",
    "| geographyname     | category   | string     | Key3 |                            |\r\n",
    "| databank          | category   | string     |      |                            |\r\n",
    "\r\n",
    "<br/>\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "The indicators that are available for all regions are limited. The rest is fragmented, most detailed data is available for the AMREG region.\r\n",
    "\r\n",
    "For the geographyid a standard applies based on the ISO 3166-1 alpha-3 standard and then extended with a 2 or 3 digit code. In order to visualize the results of the clustering in a map longitude and latitude data is needed per region. I've been only able to find the definition for it the EUREG region, but not for the other regions. This is a drawback for now. This data should be available somehow so it's not considered an impediment.\r\n",
    "\r\n",
    "## Assumptions made\r\n",
    "\r\n",
    "Though it's possible to generate cluster data on a global level it's not possible to visualize it. Therefore I've made the assumption that it's ok to take just the EUREG region so the results can be shown to the stakeholders in a map.\r\n",
    "\r\n",
    "I will focus on data that is available on a global level,but filter to the EUREG region, so that whenever the geospatial data becomes available it's easy to visualize it for all regions of the world.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering\r\n",
    "\r\n",
    "Now we have preprocessed the data we can start the clustering."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_file(par_data_dir_name, par_file_name):\r\n",
    "    X = pd.read_csv(par_data_dir_name + os.path.sep + par_file_name + '.csv',\r\n",
    "                    sep=';',\r\n",
    "                    encoding=\"utf8\")\r\n",
    "\r\n",
    "    # Dropping irrelevant columns from the data\r\n",
    "    drop_columns = [\r\n",
    "        'Unnamed: 0',\r\n",
    "        'year',\r\n",
    "        'geography_region_key',\r\n",
    "        'geographyid'\r\n",
    "    ]\r\n",
    "\r\n",
    "    X_stripped = X.drop(drop_columns, axis=1)\r\n",
    "\r\n",
    "    # Handling the missing values\r\n",
    "    X_stripped.fillna(0, inplace=True)\r\n",
    "\r\n",
    "    print(\"X.shape: \", X_stripped.shape)\r\n",
    "\r\n",
    "    return (X, X_stripped)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def do_PCA(par_X_normalized):\r\n",
    "    pca = PCA(n_components=2)\r\n",
    "    par_X_normalized = par_X_normalized.dropna()\r\n",
    "    X_principal = pca.fit_transform(par_X_normalized)\r\n",
    "    X_principal = pd.DataFrame(X_principal)\r\n",
    "    X_principal.columns = ['P1', 'P2']\r\n",
    "\r\n",
    "    return X_principal"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def init_algo():\r\n",
    "    #return DBSCAN(eps=EPS_VALUE, min_samples=MIN_SAMPLES_VALUE)\r\n",
    "    #return AffinityPropagation(random_state=None, max_iter=20)\r\n",
    "    return AgglomerativeClustering(n_clusters=N_CLUSTERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_labels(par_DBSCAN, par_X_principal):\r\n",
    "    db_default = par_DBSCAN.fit(par_X_principal)\r\n",
    "    labels = db_default.labels_\r\n",
    "    print(\"labels: \", labels.max())\r\n",
    "\r\n",
    "    return labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def generate_colours():\r\n",
    "    '''Generate a set of random colours for the plot'''\r\n",
    "\r\n",
    "    colours = {}\r\n",
    "    \r\n",
    "    for i in range(-1, 200):\r\n",
    "        r = random.random()\r\n",
    "        b = random.random()\r\n",
    "        g = random.random()\r\n",
    "        color = (r, g, b)\r\n",
    "        colours[i] = color\r\n",
    "    \r\n",
    "    return colours"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def do_plot(par_data_dir_name,\r\n",
    "            par_file_item,\r\n",
    "            par_labels,\r\n",
    "            par_X_principal,\r\n",
    "            colours):\r\n",
    "    cvec = [colours[label] for label in par_labels]\r\n",
    "\r\n",
    "    legend_list = []\r\n",
    "    label_list = []\r\n",
    "    for counter in range(0, par_labels.max()):\r\n",
    "        legend_item = plt.scatter(\r\n",
    "            par_X_principal['P1'],\r\n",
    "            par_X_principal['P2'],\r\n",
    "            color=colours[counter])\r\n",
    "        legend_list.append(legend_item)\r\n",
    "        label_item = \"Label \" + str(counter)\r\n",
    "        label_list.append(label_item)\r\n",
    "\r\n",
    "    # Plotting P1 on the X-Axis and P2 on the Y-Axis\r\n",
    "    # according to the colour vector defined\r\n",
    "    plt.figure(figsize=(9, 9))\r\n",
    "    plt.scatter(par_X_principal['P1'], par_X_principal['P2'], c=cvec)\r\n",
    "\r\n",
    "    # Building the legend\r\n",
    "    plt.legend(legend_list, label_list)\r\n",
    "\r\n",
    "    plt.savefig(par_data_dir_name + os.path.sep + par_file_item + '.png')\r\n",
    "\r\n",
    "    return plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_algo(par_algo, par_X_principal):\r\n",
    "    db = par_algo.fit(par_X_principal)\r\n",
    "\r\n",
    "    return db\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def do_clustering(par_data_dir_name, par_file_item):\r\n",
    "    X, X_stripped = read_file(par_data_dir_name, par_file_item + \"_ext2\")\r\n",
    "    X_principal = do_PCA(X_stripped)\r\n",
    "    algo = init_algo()\r\n",
    "    labels = get_labels(algo, X_principal)\r\n",
    "    result = run_algo(algo, X_principal)\r\n",
    "    plt = do_plot(par_data_dir_name,\r\n",
    "                  par_file_item,\r\n",
    "                  labels,\r\n",
    "                  X_principal,\r\n",
    "                  colours)\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    X['cluster'] = result.labels_.tolist()\r\n",
    "    X.to_csv(par_data_dir_name + os.path.sep + par_file_item + \"_clusters.csv\",\r\n",
    "             sep=\";\",\r\n",
    "             encoding=\"utf8\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\r\n",
    "\r\n",
    "The plots show the different clusters but it's not clear to which regions the data points refer.\r\n",
    "Therefore we will plot the clusterdata on a map so it's clear where the actual clusters are."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "COLOURS = [\r\n",
    "           'lightred',\r\n",
    "           'lightgreen',\r\n",
    "           'yellow',\r\n",
    "           'lightpurple',\r\n",
    "           'darkgrey',\r\n",
    "           'darkred',\r\n",
    "           'darkgreen',\r\n",
    "           'darkyellow',\r\n",
    "           'darkpurple',\r\n",
    "           'dodgerblue',\r\n",
    "           'red', \r\n",
    "           'blue',\r\n",
    "           'green',\r\n",
    "           'cyan',\r\n",
    "           'black',\r\n",
    "           'lightyellow',\r\n",
    "           'lightgrey',\r\n",
    "           'olive',\r\n",
    "           'purple',\r\n",
    "           'lime'\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_coordinates(coordinates, item_no):\r\n",
    "    if coordinates == np.nan:\r\n",
    "        return None\r\n",
    "\r\n",
    "    try:\r\n",
    "        if item_no == 0:\r\n",
    "            return coordinates[0]\r\n",
    "        else:\r\n",
    "            return coordinates[1]\r\n",
    "    except Exception:\r\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_geo_data():\r\n",
    "    DATA_FILE = \"nutspt_3.json\"\r\n",
    "    file_object = open(DATA_FILE, encoding=\"UTF-8\")\r\n",
    "    json_data = json.load(file_object)\r\n",
    "\r\n",
    "    df = pd.json_normalize(json_data['features'])\r\n",
    "\r\n",
    "    df['longitude'] = df.apply(\r\n",
    "        lambda row : get_coordinates(row['geometry.coordinates'], 0), axis = 1)\r\n",
    "    df['latitude'] = df.apply(\r\n",
    "        lambda row : get_coordinates(row['geometry.coordinates'], 1), axis = 1)\r\n",
    "    \r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def read_cluster_data(par_data_dir_name, par_file_name):\r\n",
    "    return pd.read_csv(\r\n",
    "        par_data_dir_name + os.path.sep + par_file_name + \"_clusters.csv\",\r\n",
    "        sep=\";\",\r\n",
    "        encoding=\"utf8\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def merge_data(par_df_cluster, par_df_geo):\r\n",
    "    df_cluster_merged = par_df_cluster.merge(par_df_geo,\r\n",
    "                        left_on='geographyid',\r\n",
    "                        right_on='properties.id',\r\n",
    "                        how='left')\r\n",
    "\r\n",
    "    return df_cluster_merged.dropna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_map(par_data_dir_name, par_df_cluster, par_title, par_year):\r\n",
    "  # Initialize map and center on Munich\r\n",
    "  folium_map = folium.Map(location=[48.130518, 11.5364172],\r\n",
    "                   zoom_start=3,\r\n",
    "                   width='75%',\r\n",
    "                   heigth='75%')\r\n",
    "\r\n",
    "  title_html = '''\r\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{} ({})</b></h3>\r\n",
    "             '''.format(par_title, par_year)\r\n",
    "  \r\n",
    "  folium_map.get_root().html.add_child(folium.Element(title_html))\r\n",
    "\r\n",
    "  for index, row in par_df_cluster.iterrows():\r\n",
    "    colour = COLOURS[row.cluster]\r\n",
    "    folium.CircleMarker(\r\n",
    "      location=[row['latitude'], row['longitude']],\r\n",
    "      popup=\"<stong>\" + str(row['properties.id']) + \"</stong>\",\r\n",
    "      tooltip=str(row.cluster),\r\n",
    "      color=colour,\r\n",
    "      ).add_to(folium_map)\r\n",
    "\r\n",
    "  folium_map.save(par_data_dir_name + os.path.sep + par_title + \".html\")\r\n",
    "  \r\n",
    "  folium_map\r\n",
    "\r\n",
    "  return folium_map"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def do_visualization(par_data_dir_name,\r\n",
    "                     par_file_item,\r\n",
    "                     par_df_geo_data,\r\n",
    "                     par_map_list,\r\n",
    "                     par_year):\r\n",
    "    df_cluster = read_cluster_data(par_data_dir_name, par_file_item)\r\n",
    "    df_merged = merge_data(df_cluster, par_df_geo_data)\r\n",
    "    print(\"df_merged.shape :\", df_merged.shape)\r\n",
    "    cluster_map = plot_map(par_data_dir_name,\r\n",
    "                           df_merged,\r\n",
    "                           par_file_item,\r\n",
    "                           par_year)\r\n",
    "    par_map_list.append(cluster_map)\r\n",
    "\r\n",
    "    display(cluster_map)\r\n",
    "\r\n",
    "    return par_map_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## main process \r\n",
    "\r\n",
    "Loop tthrough the different indicator and years and perform the clustering and rendering of the maps\r\n",
    "\r\n",
    "### Render cluster maps\r\n",
    "\r\n",
    "The maps are rendered per indicator per year\r\n",
    "\r\n",
    "The maps are also saved as PNG files in the data directory.\r\n",
    "\r\n",
    "### Render maps\r\n",
    "\r\n",
    "The maps are rendered per indicator per year\r\n",
    "\r\n",
    "The maps are also saved as HTML files in the data directory.\r\n",
    "\r\n",
    "It's possible to zoom in and out of an area.\r\n",
    "\r\n",
    "If you hover over a data point it shows you the cluster it belongs to. This corresponds to the cluster number as can be found in the *_cluster.csv files in the data directory.\r\n",
    "\r\n",
    "Clicking on a data point shows you the region of that data point. This corresponds to the geographyid in the *_cluster.csv files in the data directory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Download and filter base data set\r\n",
    "df_data = download_and_read_source_data()\r\n",
    "\r\n",
    "# Generate colour palette for cluster maps\r\n",
    "colours = generate_colours()\r\n",
    "\r\n",
    "# Initialize list of maps\r\n",
    "map_list = []\r\n",
    "# Retrieve geospatial data\r\n",
    "df_geo_data = read_geo_data()\r\n",
    "\r\n",
    "for file_item in FILE_LIST:\r\n",
    "    print(\"Process file: \", file_item)\r\n",
    "    for year in YEAR_LIST:\r\n",
    "        print(\">>Process year: \", year)\r\n",
    "\r\n",
    "        data_dir_name = \"data_\" + str(year)\r\n",
    "        # Create a directory for derived data.\r\n",
    "        Path(data_dir_name).mkdir(parents=True, exist_ok=True)\r\n",
    "\r\n",
    "        df_filtered = filter_data(df_data, year)\r\n",
    "\r\n",
    "        preprocess_data(data_dir_name, file_item, df_filtered)\r\n",
    "    \r\n",
    "        do_clustering(data_dir_name, file_item)\r\n",
    "    \r\n",
    "        map_list = do_visualization(data_dir_name,\r\n",
    "                                    file_item, \r\n",
    "                                    df_geo_data,\r\n",
    "                                    map_list,\r\n",
    "                                    year)\r\n",
    "\r\n",
    "print(\"End cell\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Outcome and recommendations\r\n",
    "\r\n",
    "### Justification\r\n",
    "\r\n",
    "#### Choice of algorithm\r\n",
    "\r\n",
    "The choice of algorithm is done based on research into available clustering algorithms. Algoritms like K-means are not good at handling data with outliers and handling spheres.\r\n",
    "\r\n",
    "Based on the this image it can be seen which algorithm would do a good job: [Cluster algoritm comparison](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)\r\n",
    "\r\n",
    "I've tried with the DBScan and the Agglomarative Clustering as they seem robust to some possible outcomes.\r\n",
    "My preference was for DBScan as it will determine the number of clusters itself.\r\n",
    "During the experimentation it turned out that Agglomarative Clustering gave better results than the DBScan. This could best be seen when plotting the data in the map. There you could for instance see the income differences between countries (Switzerland, Luxembourg and Norway are rich, the north of Italy is richer than the south, etc.)\r\n",
    "\r\n",
    "#### Explainable AI\r\n",
    "\r\n",
    "It's easy to put all data on a big heap and run some AI algoritm on it. This will get you a result, but it'll be difficult for a human to verify whether the result is a true result. For supervised learning this is easier to justify than for data where unsupervised learning is to be used.\r\n",
    "\r\n",
    "The setup chosen is a way that can still be understood by humans. Showing the data on the map already gives you insights. You can go back to the cluster data in the CSV files to further analyse it.\r\n",
    "\r\n",
    "### Insights\r\n",
    "\r\n",
    "Outlook from 2018 to 2024\r\n",
    "\r\n",
    "#### Household number by income band \r\n",
    "- The top segment stays about the same: Switzerland, Luxembourg,center of London and Norway.\r\n",
    "- Southern Europe shows a more diverse picture. In 2018 Italy shows a clear separation between the richer north and the poorer south. By 2024 the southern part shows more diversity in income bands.\r\n",
    "- Greece is on the rise, by 2024 it's similar to the south part of Italy, where in 2018 it was lower than that.\r\n",
    "- Germany and the Netherlands grow to be more homogeneous by 2024 as compared to 2018. This especially shows in what used to be the former DDR.\r\n",
    "- The eastern European countries show few differences between 2018 and 2014.\r\n",
    "\r\n",
    "#### Consumer spending by product\r\n",
    "- Over the years the central European countries grow to have a similar pattern. Countries like Germany, Switzerland, Austria, Czech republic and te UK are all in the same segment.\r\n",
    "- Nordic and Baltic countries show similarities, this stays the same over the years.\r\n",
    "\r\n",
    "#### Population\r\n",
    "\r\n",
    "- Where in 2018 the separation between the north and the south of Italy was evident, this difference is projected to be equalized by 2024.\r\n",
    "- Central Europe and Eastern Europe still show a clear separation by 2024 as was visible in 2018. Central Europe becomes more homogeneous as can be seen by the band running from Germany all the way down to Italy.\r\n",
    "- Greece has connected to the rest of Europe by 2024 where it was still lacking this connection in 2018.\r\n",
    "- Northwestern Europe is quite diverse and stays like that through the years.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "This script shows that it's possible to gain insights from this data set. The maps show differences in the demographics, spending behaviour and income throughout Europe. Due to the limited time available not all has been explored that can be explored. The setup of the scripts allows for an easy way to further analyze the data.\r\n",
    "\r\n",
    "### Recommendations\r\n",
    "\r\n",
    "As time was limited and some addittional data was not available not all options could be worked out. These are a few recommendations for possible next steps:\r\n",
    "\r\n",
    "- Add more indicators. The script can easily be run on other indicators.\r\n",
    "- Get geospatial data of the other regions so truly global insights can be gained. The impact of this on the script is minimal.\r\n",
    "- Try to get more data which is available for all regions or do feature engineering on existing data. Only then you can create global insights\r\n",
    "- Experiment with more/fewer clusters, this needs to be discussed with the stakeholders.\r\n",
    "- Experiment with clustering of clustering. We've a number of singular cluster topics now. If we run a clustering algorithm on the cluster data itself we could generate an aggregated clustering which could show other segements.\r\n",
    "- A more diverse choice of real swedish food in the food market... ;-) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "ac83907f397f005c98ac136081cd37806d7185ab3ba1d19778e2f4f6b6364842"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}