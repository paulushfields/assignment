{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Data Exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# FROM DATA TO INSIGHTS\r\n",
    "\r\n",
    "## Introduction\r\n",
    "This notebook is created that it should be possible to run it in one go.\r\n",
    "Python and pip should be installed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python --version"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install whatever packages that are needed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install folium\r\n",
    "!pip install matplotlib\r\n",
    "!pip install numpy\r\n",
    "!pip install pandas\r\n",
    "!pip install requests\r\n",
    "!pip install scikit-learn\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "import folium\r\n",
    "import json\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import random\r\n",
    "import requests\r\n",
    "\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "from sklearn.cluster import DBSCAN\r\n",
    "from sklearn.cluster import AffinityPropagation\r\n",
    "from sklearn.cluster import AgglomerativeClustering\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "DATA_FILE = \"global_cities_data_set.json\"\r\n",
    "URL_FILE = \"https://iisbvicmidlprdsa.blob.core.windows.net/fileshare/DATA_SET_DS_USE_CASE/global_cities_data_set.json?sv=2019-02-02&st=2021-08-06T08%3A18%3A35Z&se=2021-10-07T08%3A18%3A00Z&sr=b&sp=r&sig=vMOCDzuXhxSM%2BT02Wv3Zm2oW7BsXME2mZCk%2F%2BI5uMSU%3D\"\r\n",
    "START_FROM_SCRATCH = False\r\n",
    "\r\n",
    "# Filters\r\n",
    "REGION_FILTER = 'EUREG'\r\n",
    "YEAR_FILTER = 2023\r\n",
    "#DATA_DIR_NAME = \"data_\" + str(YEAR_FILTER)\r\n",
    "DATA_DIR_NAME = \"data_\" + \"all\"\r\n",
    "\r\n",
    "# Clustering hyper parameters\r\n",
    "EPS_VALUE = 0.02\r\n",
    "MIN_SAMPLES_VALUE = 50\r\n",
    "N_CLUSTERS = 6\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DTYPES_DICT = {\r\n",
    "    'year': np.int32,\r\n",
    "    'indicator_name': object,\r\n",
    "    'geography_iso': object,\r\n",
    "    'geography_country': object,\r\n",
    "    'geographyid': object,\r\n",
    "    'geographyname': object,\r\n",
    "    'value_unit': object,\r\n",
    "    'databank': object,\r\n",
    "    'value': np.float64\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "if START_FROM_SCRATCH:\r\n",
    "    r = requests.get(URL_FILE)\r\n",
    "    open(DATA_FILE, 'wb').write(r.content)\r\n",
    "\r\n",
    "file_object = open(DATA_FILE, encoding='utf8')\r\n",
    "data = json.load(file_object)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "# Create a directory for derived data.\r\n",
    "Path(DATA_DIR_NAME).mkdir(parents=True, exist_ok=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filtering\r\n",
    "\r\n",
    "In the current setup it's only possible to visualize the data for EU region,\r\n",
    "\r\n",
    "The geographyid is unique for all countries except for the USA. Therefore creating a combined logical key named geography_region_id consisting of geographyid and geographyname which is 100% unique for the region.\r\n",
    "year combined with geography_region_id is a primary key which can be used to merge data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "df = pd.json_normalize(data['data'])\r\n",
    "#print(df.columns.values)\r\n",
    "\r\n",
    "print(\"df.shape: (all): \", df.shape)\r\n",
    "# Make sure the year field is an integer\r\n",
    "df.year = df.year.astype('int32')\r\n",
    "#rint(df.dtypes)\r\n",
    "\r\n",
    "df[\"geography_region_key\"] = df[\"geographyid\"] + \"_\" + df[\"geographyname\"]\r\n",
    "#df = df[(df['databank'] == 'EUREG') & (df['year'] == YEAR_FILTER)]\r\n",
    "#print(\"df.shape: (\" + REGION_FILTER +  \" & \" + str(YEAR_FILTER) +  \"): \", df.shape)\r\n",
    "df = df[(df['databank'] == 'EUREG')]\r\n",
    "print(\"df.shape: (\" + REGION_FILTER + \"): \", df.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "df.shape: (all):  (894942, 9)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indicators\r\n",
    "\r\n",
    "The file provided hosts a number of different types of data as can be seen in the indicator_name field.\r\n",
    "Some indicators belong together. For example Population per age range.\r\n",
    "These indicator_groups are handled separately.\r\n",
    "\r\n",
    "Singular indicator are written into separate files."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "\r\n",
    "#Some indicator are grouped\r\n",
    "indicator_groups = [\r\n",
    "    'Household numbers by income band',\r\n",
    "    'Population',\r\n",
    "    'Consumer spending by product'\r\n",
    "]\r\n",
    "\r\n",
    "indicator_groups_strings = (\r\n",
    "    'Household numbers by income band',\r\n",
    "    'Population',\r\n",
    "    'Consumer spending by product'\r\n",
    ")\r\n",
    "\r\n",
    "other_indicators = []\r\n",
    "\r\n",
    "for word in df.indicator_name.unique()[:]:\r\n",
    "    if not word.startswith(indicator_groups_strings):\r\n",
    "        other_indicators.append(word)\r\n",
    "\r\n",
    "# Create separate files for indicators.\r\n",
    "for indicator in other_indicators:\r\n",
    "    df_filtered = df[(df['indicator_name'] == indicator)]\r\n",
    "    filtered_file_name = DATA_DIR_NAME + os.path.sep + indicator.replace(\" \", \"_\"). \\\r\n",
    "       replace(\",\", \"_\").replace(\"/\", \"_\") + '.csv'\r\n",
    "    df_filtered.to_csv(filtered_file_name, sep=\";\", encoding=\"utf-8\")\r\n",
    "\r\n",
    "# Group some indicators into one file.\r\n",
    "for indicator_group in indicator_groups:\r\n",
    "    df_filtered = df[(df['indicator_name'].str.startswith(indicator_group))]\r\n",
    "    filtered_file_name = DATA_DIR_NAME + os.path.sep + indicator_group + '.csv'\r\n",
    "    df_filtered.to_csv(filtered_file_name, sep=\";\", encoding=\"utf-8\")\r\n",
    "\r\n",
    "df.to_csv(DATA_DIR_NAME + os.path.sep + \"total_set.csv\", sep=\";\", encoding=\"utf-8\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "file_object.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Indicator groups\r\n",
    "\r\n",
    "Now process the indicator groups. Different bands of the same kind of data are put into one file for further processing.\r\n",
    "\r\n",
    "As the value_unit might not be the same we can't compare the data is that original form.\r\n",
    "For each band a ratio is calculated to indicate what proportion of total this band represents.\r\n",
    "This makes it possible to compare the data no matter the country."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "file_list = [\r\n",
    "    'Consumer spending by product',\r\n",
    "    'Population',\r\n",
    "    'Household numbers by income band'\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "for file_item in file_list:\r\n",
    "    df_data = pd.read_csv(\r\n",
    "        DATA_DIR_NAME + os.path.sep + file_item + \".csv\",\r\n",
    "        sep=\";\",\r\n",
    "        encoding=\"utf8\",\r\n",
    "        dtypes=DTYPES_DICT)\r\n",
    "\r\n",
    "    print(\"shape: \", df_data.shape)\r\n",
    "\r\n",
    "    # Remove unwanted columns when grouping\r\n",
    "    df_sum = df_data.loc[:, (\"geography_region_key\", \"year\", \"value\")]\r\n",
    "\r\n",
    "    # Sum values\r\n",
    "    df_grouped = df_sum.groupby(by=['year', 'geography_region_key']).sum()\r\n",
    "    # Back to a data frame\r\n",
    "    df_sum = df_grouped.reset_index()\r\n",
    "\r\n",
    "    def calculate_ratio(par_year, par_geography_region_key, par_value):\r\n",
    "        df_filtered_sum = df_sum[(df_sum['year'] == par_year) &\r\n",
    "            (df_sum['geography_region_key'] == par_geography_region_key)].sum()\r\n",
    "        return par_value / df_filtered_sum.values[2]\r\n",
    "\r\n",
    "    df_data['ratio'] = df_data.apply(\r\n",
    "            lambda row : calculate_ratio(\r\n",
    "                row['year'],\r\n",
    "                row['geography_region_key'],\r\n",
    "                row['value']), axis = 1)\r\n",
    "\r\n",
    "    df_data['ratio'].fillna(0, inplace=True)\r\n",
    "    \r\n",
    "    print(\"shape: \", df_data.shape)\r\n",
    "\r\n",
    "    df_data.to_csv(DATA_DIR_NAME + os.path.sep + file_item + \"_ext.csv\",\r\n",
    "        sep=\";\",\r\n",
    "        encoding=\"utf8\")\r\n",
    "\r\n",
    "    print(\"End \" + file_item)\r\n",
    "\r\n",
    "print(\"End cell\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\jvandervelden\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape:  (225109, 11)\n",
      "shape:  (225109, 12)\n",
      "End Consumer spending by product\n",
      "shape:  (270232, 11)\n",
      "shape:  (270232, 12)\n",
      "End Population\n",
      "shape:  (237010, 11)\n",
      "shape:  (237010, 12)\n",
      "End Household numbers by income band\n",
      "End cell\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "for file_item in file_list:\r\n",
    "    df_data = pd.read_csv(\r\n",
    "        DATA_DIR_NAME + os.path.sep + file_item + \"_ext.csv\",\r\n",
    "        sep=\";\",\r\n",
    "        encoding=\"utf8\")\r\n",
    "\r\n",
    "    print(\"shape: \", df_data.shape)\r\n",
    "\r\n",
    "    column_names = []\r\n",
    "    df_data_ext = pd.DataFrame()\r\n",
    "\r\n",
    "    # Create single data points\r\n",
    "    indicator_names = df_data.indicator_name.unique()\r\n",
    "    for indicator_name in indicator_names:\r\n",
    "        df_select = df_data[df_data.indicator_name == indicator_name] \r\n",
    "        column_name = indicator_name. \\\r\n",
    "            replace(\"resident\", \"\"). \\\r\n",
    "            replace(\"based\", \"\"). \\\r\n",
    "            replace(\"current\", \"\"). \\\r\n",
    "            replace(\"prices\", \"\"). \\\r\n",
    "            replace(\"(\", \"\"). \\\r\n",
    "            replace(\")\", \"\"). \\\r\n",
    "            replace(\"Consumer spending by product / service - \", \"\"). \\\r\n",
    "            replace(\"Household numbers by income band - \", \"\"). \\\r\n",
    "            replace(\",\", \"\"). \\\r\n",
    "            replace(\" \", \"_\"). \\\r\n",
    "            replace(\"-\", \"_\"). \\\r\n",
    "            replace(\"____\", \"\"). \\\r\n",
    "            replace(\"__\", \"_\"). \\\r\n",
    "            lower()\r\n",
    "        print(\"column_name: \", column_name)\r\n",
    "        column_names.append(column_name)\r\n",
    "        df_select[column_name] = df_select['ratio']\r\n",
    "        df_select = df_select.loc[:, (\"geographyid\", \"geography_region_key\", \"year\", column_name)]\r\n",
    "\r\n",
    "        if (len(df_data_ext) == 0):\r\n",
    "            df_data_ext = df_select\r\n",
    "        else:\r\n",
    "            df_data_ext = df_data_ext.merge(\r\n",
    "                right=df_select,\r\n",
    "                on=[\"geographyid\", \"geography_region_key\", \"year\"],\r\n",
    "                how=\"outer\")\r\n",
    "\r\n",
    "        print(\"Shape: \", df_data_ext.shape)\r\n",
    "\r\n",
    "    df_data_ext.fillna(0, inplace=True)\r\n",
    "\r\n",
    "    df_data_ext.to_csv(DATA_DIR_NAME + os.path.sep + file_item + \"_ext2.csv\",\r\n",
    "        sep=\";\",\r\n",
    "        encoding=\"utf8\")\r\n",
    "\r\n",
    "    print(\"End \" + file_item)\r\n",
    "                \r\n",
    "print(\"End cell\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "shape:  (225109, 13)\n",
      "column_name:  furniture_and_furnishings_carpets_and_other_floor_coverings\n",
      "Shape:  (18272, 4)\n",
      "column_name:  household_and_garden_tools_and_equipment\n",
      "Shape:  (18272, 5)\n",
      "column_name:  household_appliances\n",
      "Shape:  (18272, 6)\n",
      "column_name:  household_furnishings_household_equipment_and_other_housing_expenditure__total\n",
      "Shape:  (18272, 7)\n",
      "column_name:  household_glassware_tableware_and_household_utensils\n",
      "Shape:  (18272, 8)\n",
      "column_name:  household_textiles\n",
      "Shape:  (18272, 9)\n",
      "column_name:  routine_household_maintenance_goods_and_services\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-107-ac09588ee215>:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_select[column_name] = df_select['ratio']\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Shape:  (18272, 10)\n",
      "column_name:  audio_visual_photographic_and_information_processing_equipment\n",
      "Shape:  (18272, 11)\n",
      "column_name:  eating_out\n",
      "Shape:  (18272, 12)\n",
      "column_name:  education\n",
      "Shape:  (18272, 13)\n",
      "column_name:  housing_rent\n",
      "Shape:  (18272, 14)\n",
      "column_name:  imputed_housing_rent\n",
      "Shape:  (18272, 15)\n",
      "column_name:  newspapers_books_and_stationery\n",
      "Shape:  (18272, 16)\n",
      "column_name:  other_recreational_and_cultural_durable_goods\n",
      "Shape:  (18272, 17)\n",
      "column_name:  housing_maintenance_and_repairs\n",
      "Shape:  (18272, 18)\n",
      "column_name:  housing_water_electricity_gas_and_other_fuels__total\n",
      "Shape:  (18272, 19)\n",
      "column_name:  housing_electricity_gas_and_other_fuels\n",
      "Shape:  (18272, 20)\n",
      "column_name:  hospital_services\n",
      "Shape:  (18272, 21)\n",
      "column_name:  housing_water_charges\n",
      "Shape:  (18272, 22)\n",
      "column_name:  insurance\n",
      "Shape:  (18272, 23)\n",
      "column_name:  other_recreational_items_and_equipment\n",
      "Shape:  (18272, 24)\n",
      "column_name:  total_consumer_spending\n",
      "Shape:  (18272, 25)\n",
      "column_name:  personal_transport_running_costs\n",
      "Shape:  (18272, 26)\n",
      "column_name:  social_protection\n",
      "Shape:  (18272, 27)\n",
      "column_name:  personal_care_goods_and_services\n",
      "Shape:  (18272, 28)\n",
      "column_name:  vehicle_purchases\n",
      "Shape:  (18272, 29)\n",
      "End Consumer spending by product\n",
      "shape:  (270232, 13)\n",
      "column_name:  population_45_49\n",
      "Shape:  (18256, 4)\n",
      "column_name:  population_25_29\n",
      "Shape:  (18256, 5)\n",
      "column_name:  population_10_14\n",
      "Shape:  (18336, 6)\n",
      "column_name:  population_70_74\n",
      "Shape:  (18336, 7)\n",
      "column_name:  population_65_69\n",
      "Shape:  (18336, 8)\n",
      "column_name:  population_55_59\n",
      "Shape:  (18336, 9)\n",
      "column_name:  population_60_64\n",
      "Shape:  (18336, 10)\n",
      "column_name:  population_80+\n",
      "Shape:  (18336, 11)\n",
      "column_name:  population_40_44\n",
      "Shape:  (18336, 12)\n",
      "column_name:  population_75_79\n",
      "Shape:  (18336, 13)\n",
      "column_name:  population_35_39\n",
      "Shape:  (18336, 14)\n",
      "column_name:  population_20_24\n",
      "Shape:  (18336, 15)\n",
      "column_name:  population_50_54\n",
      "Shape:  (18336, 16)\n",
      "column_name:  population_15_19\n",
      "Shape:  (18336, 17)\n",
      "column_name:  population_0_4\n",
      "Shape:  (18336, 18)\n",
      "column_name:  population_30_34\n",
      "Shape:  (18336, 19)\n",
      "column_name:  population_15_19\n",
      "Shape:  (18336, 20)\n",
      "column_name:  population_20_24\n",
      "Shape:  (18336, 21)\n",
      "column_name:  population_25_29\n",
      "Shape:  (18336, 22)\n",
      "column_name:  population_30_34\n",
      "Shape:  (18336, 23)\n",
      "column_name:  population_35_39\n",
      "Shape:  (18336, 24)\n",
      "column_name:  population_40_44\n",
      "Shape:  (18336, 25)\n",
      "column_name:  population_45_49\n",
      "Shape:  (18336, 26)\n",
      "column_name:  population_50_54\n",
      "Shape:  (18336, 27)\n",
      "column_name:  population_60_64\n",
      "Shape:  (18336, 28)\n",
      "End Population\n",
      "shape:  (237010, 13)\n",
      "column_name:  $20000–35000\n",
      "Shape:  (18336, 4)\n",
      "column_name:  $35000–70000\n",
      "Shape:  (18336, 5)\n",
      "column_name:  $150000–200000\n",
      "Shape:  (18336, 6)\n",
      "column_name:  over_$250000\n",
      "Shape:  (18416, 7)\n",
      "column_name:  $2000–5000\n",
      "Shape:  (18416, 8)\n",
      "column_name:  up_to_$1000\n",
      "Shape:  (18416, 9)\n",
      "column_name:  $1000–2000\n",
      "Shape:  (18416, 10)\n",
      "column_name:  $10000–20000\n",
      "Shape:  (18416, 11)\n",
      "column_name:  $70000–100000\n",
      "Shape:  (18416, 12)\n",
      "column_name:  $5000–7500\n",
      "Shape:  (18416, 13)\n",
      "column_name:  $7500–10000\n",
      "Shape:  (18416, 14)\n",
      "column_name:  $200000–$250000\n",
      "Shape:  (18416, 15)\n",
      "column_name:  $100000–150000\n",
      "Shape:  (18416, 16)\n",
      "End Household numbers by income band\n",
      "End cell\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data exploration\r\n",
    "\r\n",
    "Now we have a set of different files, one file for each indicator(group). Let's look at the data in more detail.\r\n",
    "\r\n",
    "### Primary data points\r\n",
    "\r\n",
    "Some of the data are the primary datapoints. These can be divided into grouped and non-grouped indicators.\r\n",
    "\r\n",
    "\r\n",
    "### Non-grouped indicators:\r\n",
    "| indicator_name                                                        | indicator_type | value_unit      | value_type | regions          | comment                        |   |\r\n",
    "|-----------------------------------------------------------------------|-----------------|-----------------|------------|------------------|--------------------------------|---|\r\n",
    "| Average_household_size                                                | demographics    | #Persons        | float      | AFR, EUREG, GCFS |                                |   |\r\n",
    "| Births                                                                | demographics    | #Persons        | float      | AFR, GCFS        | how to interpret? Aggregations |   |\r\n",
    "| CREA_house_price_index                                                | housing         | Index           | float      | AMREG            | CAN                            |   |\r\n",
    "| Deaths                                                                | demographics    | #Persons        | float      | AFR, GCFS        | how to interpret?              |   |\r\n",
    "| Employment_-_Industry                                                 | employment      | #Persons        | float      | AFR, GCFS        | not complete, how to interpret |   |\r\n",
    "| Employment_-_Transport__storage__information_&_communication_services | employment      | #Persons        | float      | AFR, GCFS        | how to interpret, not complete |   |\r\n",
    "| Gross_domestic_product__real                                          | gdp             | currency        | float      | EUREG, AMREG     |                                |   |\r\n",
    "| Homeownership_rate                                                    | housing         | %               | float      | AMREG            | USA                            |   |\r\n",
    "| Household_disposable_income__per_household__nominal                   | housing         | currency        | float      | EUREG            |                                |   |\r\n",
    "| Household_disposable_income__per_household__real                      | housing         | currency        | float      | EUREG            |                                |   |\r\n",
    "| Household_disposable_income__real                                     | housing         | currency        | float      | EUREG            |                                |   |\r\n",
    "| Housing_permits_-_multi_family                                        | housing         | Housing permits | float      | AMREG            | USA                            |   |\r\n",
    "| Housing_permits_-_single_family                                       | housing         | Housing permits | float      | AMREG            | USA                            |   |\r\n",
    "| Housing_permits_-_total                           | housing      | Housing permits | float | AMREG     | USA                               |   |\r\n",
    "| Housing_starts                                    | housing      | null            | float | AMREG     | CAN, how to interpret?            |   |\r\n",
    "| Housing_starts_-_multi_family                     | housing      | Housing starts  | float | AMREG     | USA                               |   |\r\n",
    "| Housing_starts_-_single_family                    | housing      | Housing starts  | float | AMREG     | USA                               |   |\r\n",
    "| Housing_starts_-_total                            | housing      | Housing starts  | float | AMREG     | USA                               |   |\r\n",
    "| Income_from_employment__nominal                   | income       | currency        | float | AMREG     | USA                               |   |\r\n",
    "| Income_from_rent__dividends_and_interest__nominal | income       | currency        | float | AMREG     | USA                               |   |\r\n",
    "| Income_taxes__nominal                             | income       | currency        | float | AMREG     | USA                               |   |\r\n",
    "| Labor_force                                       | employment   | #Persons        | float | AMREG     | USA, CAN                          |   |\r\n",
    "| Labor_force_participation_rate                    | employment   | %               | float | AMREG     | USA                               |   |\r\n",
    "| Labour_force_participation_rate                   | employment   | %               | float | AMREG     | CAN                               |   |\r\n",
    "| Median_household_income__real                     | income       | currency        | float | AMREG     | USA                               |   |\r\n",
    "| Net_migration_(including_statistical_adjustment)  | demographics | #Persons        | float | AFR, GCFS | can be both negative and positive |   |\r\n",
    "| New_housing_price_index                           | housing      | index           | float | AMREG     | CAN                               |   |\r\n",
    "| Personal_disposable_income__per_capita__real      | income       | currency        | float | AMREG     | USA, CAN                          |   |\r\n",
    "| Personal_disposable_income__per_household__real   | income       | currency        | float | AMREG     | USA, CAN                          |   |\r\n",
    "| Personal_income__per_capita__real    | income       | currency    | float | AMREG | USA, CAN |   |\r\n",
    "| Personal_income__per_household__real | income       | currency    | float | AMREG | USA, CAN |   |\r\n",
    "| Proprietors_incomes__nominal         | income       | currency    | float | AMREG | USA      |   |\r\n",
    "| Residential_building_permits         | housing      | null        | float | AMREG | CAN      |   |\r\n",
    "| Social_security_payments__nominal    | income       | currency    | float | AMREG | USA      |   |\r\n",
    "| Total_households                     | housing      | #Households | float | All   |          |   |\r\n",
    "| Total_population                     | demographics | #Persons    | float | All   |          |   |\r\n",
    "| Unemployment_level                   | unemployment | #Persons    | float | AMREG | USA, CAN |   |\r\n",
    "| Unemployment_rate                    | unemployment | %           | float | AMREG | USA, CAN |   |\r\n",
    "| Urban_Total_Population               | demographics | #Persons    | float | All   |          |   |\r\n",
    "\r\n",
    "<br/>\r\n",
    "\r\n",
    "### Grouped indicators\r\n",
    "\r\n",
    "| indicator_name                    | indicator_type  | value_unit  | value_type | regions | comment                                             |\r\n",
    "|-----------------------------------|-----------------|-------------|------------|---------|-----------------------------------------------------|\r\n",
    "| Population*                       | demographics    | #Persons    | float      | All     |                                                     |\r\n",
    "| Consumer spending by product*     | spending        | currency    | float      | All     | value_unit contains : empty, null                   |\r\n",
    "| Household numbers by income band* | income          | #Households | float      | All     | value contains float values very big and very small |\r\n",
    "\r\n",
    "<br/>\r\n",
    "\r\n",
    "## Secondary data points\r\n",
    "\r\n",
    "There's a set of secondary data points that describe the primary data points in terms of a number of facets. For instance geographical region, year etc.\r\n",
    "\r\n",
    "| indicator_name    | value_unit | value_type | key  | comment                    |\r\n",
    "|-------------------|------------|------------|------|----------------------------|\r\n",
    "| year              | year       | int        | Key1 |                            |\r\n",
    "| geography_iso     | category   | string     |      | ISO 3166-1 alpha-3         |\r\n",
    "| geography_country | category   | string     |      |                            |\r\n",
    "| geographyid       | category   | string     | Key2 | NUTS-2 region data (EUREG), No standards found for other regions |\r\n",
    "| geographyname     | category   | string     | Key3 |                            |\r\n",
    "| databank          | category   | string     |      |                            |\r\n",
    "\r\n",
    "<br/>\r\n",
    "\r\n",
    "## Conclusion\r\n",
    "\r\n",
    "The indicators that are available for all regions are limited. The rest is fragmented, most detailed of data is available for the AMREG region.\r\n",
    "\r\n",
    "For the geographyid a standard applies based on the ISO 3166-1 alpha-3 and then extended with a 2 or 3 digit code. In order to visualize the results of the clustering in a map longitude and latitude data is needed per region. I've been only able to find the definition for it the EUREG region, but not for the other regions. This is a major drawback for now. This data should be available somehow so it's not considered an impediment for now.\r\n",
    "\r\n",
    "## Assumptions made\r\n",
    "\r\n",
    "Though it's possible to generate cluster data on a global level it's not possible to visualize it. Therefore I've taken the assumption here that it's ok to take just the EUREG region so the results can be shown to the stakeholders.\r\n",
    "\r\n",
    "I will focus on data that is available on a global level, so that whenever the geospatial data becomes available it's easy to visualize it for all regions of the world.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clustering\r\n",
    "\r\n",
    "Now we have preprocessed the data we can start the clustering."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "def read_file(file_name):\r\n",
    "    X = pd.read_csv(DATA_DIR_NAME + os.path.sep + file_name + '.csv',\r\n",
    "                    sep=';',\r\n",
    "                    encoding=\"utf8\")\r\n",
    "\r\n",
    "    # Dropping irrelevant columns from the data\r\n",
    "    drop_columns = [\r\n",
    "        'Unnamed: 0',\r\n",
    "        'year',\r\n",
    "        'geography_region_key',\r\n",
    "        'geographyid'\r\n",
    "    ]\r\n",
    "\r\n",
    "    X_stripped = X.drop(drop_columns, axis=1)\r\n",
    "\r\n",
    "    # Handling the missing values\r\n",
    "    X_stripped.fillna(0, inplace=True)\r\n",
    "\r\n",
    "    print(\"X.shape: \", X_stripped.shape)\r\n",
    "\r\n",
    "    return (X, X_stripped)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "def do_PCA(par_X_normalized):\r\n",
    "    pca = PCA(n_components=2)\r\n",
    "    par_X_normalized = par_X_normalized.dropna()\r\n",
    "    X_principal = pca.fit_transform(par_X_normalized)\r\n",
    "    X_principal = pd.DataFrame(X_principal)\r\n",
    "    X_principal.columns = ['P1', 'P2']\r\n",
    "    #print(\"PCA\")\r\n",
    "    #print(X_principal.head())\r\n",
    "\r\n",
    "    return X_principal"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "def init_algo():\r\n",
    "    #return DBSCAN(eps=EPS_VALUE, min_samples=MIN_SAMPLES_VALUE)\r\n",
    "    #return AffinityPropagation(random_state=None, max_iter=20)\r\n",
    "    return AgglomerativeClustering(n_clusters=N_CLUSTERS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "def get_labels(par_DBSCAN, par_X_principal):\r\n",
    "    # Numpy array of all the cluster labels assigned to each data point\r\n",
    "    db_default = par_DBSCAN.fit(par_X_principal)\r\n",
    "    labels = db_default.labels_\r\n",
    "    print(\"labels: \", labels.max())\r\n",
    "\r\n",
    "    return labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "def generate_colours():\r\n",
    "    '''Generate a set of random colours for the plot'''\r\n",
    "\r\n",
    "    colours = {}\r\n",
    "    \r\n",
    "    for i in range(-1, 200):\r\n",
    "        r = random.random()\r\n",
    "        b = random.random()\r\n",
    "        g = random.random()\r\n",
    "        color = (r, g, b)\r\n",
    "        colours[i] = color\r\n",
    "    \r\n",
    "    return colours"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "def do_plot(par_labels, par_X_principal, colours):\r\n",
    "    # Building the colour vector for each data point\r\n",
    "    cvec = [colours[label] for label in par_labels]\r\n",
    "\r\n",
    "    legend_list = []\r\n",
    "    label_list = []\r\n",
    "    print(\"#labels: \", par_labels.max())\r\n",
    "    for counter in range(0, par_labels.max()):\r\n",
    "        # For the construction of the legend of the plot\r\n",
    "        legend_item = plt.scatter(\r\n",
    "            par_X_principal['P1'],\r\n",
    "            par_X_principal['P2'],\r\n",
    "            color=colours[counter])\r\n",
    "        legend_list.append(legend_item)\r\n",
    "        label_item = \"Label \" + str(counter)\r\n",
    "        label_list.append(label_item)\r\n",
    "\r\n",
    "    # Plotting P1 on the X-Axis and P2 on the Y-Axis\r\n",
    "    # according to the colour vector defined\r\n",
    "    plt.figure(figsize =(9, 9))\r\n",
    "    plt.scatter(par_X_principal['P1'], par_X_principal['P2'], c=cvec)\r\n",
    "\r\n",
    "    # Building the legend\r\n",
    "    plt.legend(legend_list, label_list)\r\n",
    "\r\n",
    "    return plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "def run_algo(par_algo, par_X_principal):\r\n",
    "    db = par_algo.fit(par_X_principal)\r\n",
    "\r\n",
    "    return db\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "colours = generate_colours()\r\n",
    "for file_item in file_list:\r\n",
    "    print(\"file_item: \" + file_item)\r\n",
    "    X, X_stripped = read_file(file_item + \"_ext2\")\r\n",
    "    #X_normalized = normalize(X)\r\n",
    "    X_principal = do_PCA(X_stripped)\r\n",
    "    algo = init_algo()\r\n",
    "    labels = get_labels(algo, X_principal)\r\n",
    "    result = run_algo(algo, X_principal)\r\n",
    "    plt = do_plot(labels, X_principal, colours)\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "    X['cluster'] = result.labels_.tolist()\r\n",
    "    X.to_csv(DATA_DIR_NAME + os.path.sep + file_item + \"_clusters.csv\",\r\n",
    "             sep=\";\",\r\n",
    "             encoding=\"utf8\")\r\n",
    "\r\n",
    "    print(\"End \" + file_item)\r\n",
    "\r\n",
    "print (\"End cell\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "file_item: Consumer spending by product\n",
      "X.shape:  (18272, 27)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'AT111'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-433d8707a123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_stripped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_item\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"_ext2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#X_normalized = normalize(X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mX_principal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_PCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_stripped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0malgo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_algo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malgo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_principal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-109-b1fd85a293a9>\u001b[0m in \u001b[0;36mdo_PCA\u001b[1;34m(par_X_normalized)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpar_X_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpar_X_normalized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX_principal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpar_X_normalized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mX_principal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_principal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mX_principal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'P1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'P2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mC\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse\u001b[0m \u001b[1;34m'np.ascontiguousarray'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \"\"\"\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    395\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         X = self._validate_data(X, dtype=[np.float64, np.float32],\n\u001b[0m\u001b[0;32m    398\u001b[0m                                 ensure_2d=True, copy=self.copy)\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'no_validation'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'AT111'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization\r\n",
    "\r\n",
    "As the plots do show the different clusters it's not clear to which regions the data points refer.\r\n",
    "Therefore we will plot the clusterdata on a map so it's clear where the actual clusters are."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "COLOURS = [\r\n",
    "           'lightred',\r\n",
    "           'lightgreen',\r\n",
    "           'lightyellow',\r\n",
    "           'lightpurple',\r\n",
    "           'darkgrey',\r\n",
    "           'darkred',\r\n",
    "           'darkgreen',\r\n",
    "           'darkyellow',\r\n",
    "           'darkpurple',\r\n",
    "           'dodgerblue',\r\n",
    "           'red', \r\n",
    "           'blue',\r\n",
    "           'green',\r\n",
    "           'cyan',\r\n",
    "           'black',\r\n",
    "           'yellow',\r\n",
    "           'lightgrey',\r\n",
    "           'olive',\r\n",
    "           'purple',\r\n",
    "           'lime'\r\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "def get_coordinates(coordinates, item_no):\r\n",
    "    if coordinates == np.nan:\r\n",
    "        return None\r\n",
    "\r\n",
    "    try:\r\n",
    "        if item_no == 0:\r\n",
    "            return coordinates[0]\r\n",
    "        else:\r\n",
    "            return coordinates[1]\r\n",
    "    except Exception:\r\n",
    "        return None"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "def read_geo_data():\r\n",
    "    DATA_FILE = \"nutspt_3.json\"\r\n",
    "    file_object = open(DATA_FILE, encoding=\"UTF-8\")\r\n",
    "    json_data = json.load(file_object)\r\n",
    "\r\n",
    "    df = pd.json_normalize(json_data['features'])\r\n",
    "\r\n",
    "    df['longitude'] = df.apply(\r\n",
    "        lambda row : get_coordinates(row['geometry.coordinates'], 0), axis = 1)\r\n",
    "    df['latitude'] = df.apply(\r\n",
    "        lambda row : get_coordinates(row['geometry.coordinates'], 1), axis = 1)\r\n",
    "    \r\n",
    "    df.to_csv(DATA_DIR_NAME + os.path.sep + \"nutspt_3.csv\",\r\n",
    "          sep=\";\",\r\n",
    "          encoding='utf8')\r\n",
    "\r\n",
    "    return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "def read_cluster_data(file_name):\r\n",
    "    return pd.read_csv(DATA_DIR_NAME + os.path.sep + file_name + \"_clusters.csv\",\r\n",
    "                        sep=\";\",\r\n",
    "                        encoding=\"utf8\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "def merge_data(par_df_cluster, par_df_geo):\r\n",
    "    df_cluster_merged = par_df_cluster.merge(par_df_geo,\r\n",
    "                        left_on='geography_id',\r\n",
    "                        right_on='properties.id',\r\n",
    "                        how='left')\r\n",
    "\r\n",
    "    return df_cluster_merged.dropna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "def plot_map(par_df_cluster, par_title):\r\n",
    "  # Initialize map and center on Munich\r\n",
    "  folium_map = folium.Map(location=[48.130518, 11.5364172],\r\n",
    "                   zoom_start=3,\r\n",
    "                   width='75%',\r\n",
    "                   heigth='75%')\r\n",
    "\r\n",
    "  title_html = '''\r\n",
    "             <h3 align=\"center\" style=\"font-size:16px\"><b>{}</b></h3>\r\n",
    "             '''.format(par_title)\r\n",
    "  \r\n",
    "  folium_map.get_root().html.add_child(folium.Element(title_html))\r\n",
    "\r\n",
    "  for index, row in par_df_cluster.iterrows():\r\n",
    "    colour = COLOURS[row.cluster]\r\n",
    "    folium.CircleMarker(\r\n",
    "      location=[row['latitude'], row['longitude']],\r\n",
    "      popup=\"<stong>\" + str(row['properties.id']) + \"</stong>\",\r\n",
    "      tooltip=str(row.cluster),\r\n",
    "      color=colour,\r\n",
    "      ).add_to(folium_map)\r\n",
    "\r\n",
    "  return folium_map"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "map_list = []\r\n",
    "df_geo_data = read_geo_data()\r\n",
    "for file_item in file_list:\r\n",
    "    df_cluster = read_cluster_data(file_item)\r\n",
    "    df_merged = merge_data(df_cluster, df_geo_data)\r\n",
    "    print(\"df_merged.shape :\", df_merged.shape)\r\n",
    "    cluster_map = plot_map(df_merged, file_item)\r\n",
    "    map_list.append(cluster_map)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 3, placement implies 1",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'geography_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3825\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3826\u001b[1;33m             \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3827\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'geography_id'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-7af8022d3b73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf_geo_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_geo_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile_item\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdf_cluster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_cluster_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_item\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdf_merged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cluster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_geo_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df_merged.shape :\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-104-d5e9520d20de>\u001b[0m in \u001b[0;36mread_cluster_data\u001b[1;34m(file_name)\u001b[0m\n\u001b[0;32m      4\u001b[0m                         encoding=\"utf8\")\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'geography_id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'geography_region_key'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     df.to_csv(DATA_DIR_NAME + os.path.sep + file_name + \"_clusters2.csv\",\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3161\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3162\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3163\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3165\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3241\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3242\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3243\u001b[1;33m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3245\u001b[0m         \u001b[1;31m# check if we are modifying a copy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3827\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3828\u001b[0m             \u001b[1;31m# This item wasn't present, just insert at end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3829\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3830\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[0;32m   1201\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_reshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         \u001b[0mblock\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblknos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[0;32m   2740\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatetimeArray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2742\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2743\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_ndim\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    143\u001b[0m                 \u001b[1;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 3, placement implies 1"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map_list[0]"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-b9edaca4b94d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmap_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map_list[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "map_list[2]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit (conda)"
  },
  "interpreter": {
   "hash": "44ef153a9fbb57cb3fb6660b3bd49de460b2303e9881c96f73e7f40a52de276e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}